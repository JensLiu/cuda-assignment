#include <cub/cub.cuh>
#include <cuda_runtime.h>
#include <iostream>
#include <vector>

#ifndef NumElem
#define NumElem 512
#endif

#include <sys/resource.h>
#include <sys/times.h>

float GetTime(void) {
  struct timeval tim;
  struct rusage ru;
  getrusage(RUSAGE_SELF, &ru);
  tim = ru.ru_utime;
  return ((float)tim.tv_sec + (float)tim.tv_usec / 1000000.0) * 1000.0;
}

__global__ void sum_kernel(const double *in, double *out, int N) {
  using BlockReduce = cub::BlockReduce<double, NumElem>;
  __shared__ typename BlockReduce::TempStorage temp_storage;

  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  double val = (idx < N) ? in[idx] : 0;

  double blockAggregated = BlockReduce(temp_storage).Sum(val);

  if (threadIdx.x == 0) {
    out[blockIdx.x] = blockAggregated;
  }
}

void InitV(int N, double *v);
int Test(int N, double *v, double sum, double *res);

int main() {
  unsigned int N;
  unsigned int numBytesV, numBytesW;
  unsigned int nBlocks, nThreads;
  int test;
  float SeqTime, elapsedTime;
  float t1, t2;

  cudaEvent_t start, stop;

  double *h_v, *h_w;
  double *d_v, *d_w;

  double SUM, SumSeq;
  int i;
  int count, gpu, tmp;

  N = 1024 * 1024 * 16;
  nThreads = NumElem; // This value must match the number of elements handled by
                      // the kernel
  nBlocks = (N + nThreads - 1) / nThreads;

  numBytesV = N * sizeof(double);
  numBytesW = nBlocks * sizeof(double);

  // Select GPU randomly
  cudaGetDeviceCount(&count);
  srand(time(NULL));
  tmp = rand();
  gpu = (tmp >> 3) % count;
  cudaSetDevice(gpu);

  cudaEventCreate(&start);
  cudaEventCreate(&stop);

  // Allocate memory on the host
  h_v = (double *)malloc(numBytesV);
  h_w = (double *)malloc(numBytesW);

  // Obtain [pinned] memory on the host
  // cudaMallocHost((double**)&h_v, numBytesV);
  // cudaMallocHost((double**)&h_w, numBytesW);

  // Initialize the vectors
  InitV(N, h_v);

  // Allocate memory on the device
  cudaMalloc((double **)&d_v, numBytesV);
  cudaMalloc((double **)&d_w, numBytesW);

  // Copy data from host to device
  cudaMemcpy(d_v, h_v, numBytesV, cudaMemcpyHostToDevice);

  cudaEventRecord(start, 0);

  // Run the kernel
  sum_kernel<<<nBlocks, nThreads>>>(d_v, d_w, N);

  // Copy the partial result back to the host
  cudaMemcpy(h_w, d_w, numBytesW, cudaMemcpyDeviceToHost);

  SUM = 0.0;
  for (i = 0; i < nBlocks; i++)
    SUM = SUM + h_w[i];

  cudaEventRecord(stop, 0);
  cudaEventSynchronize(stop);

  // Free device memory
  cudaFree(d_v);
  cudaFree(d_w);

  cudaEventElapsedTime(&elapsedTime, start, stop);
  printf("\nKERNEL 09\n");
  printf("GPU used: %d\n", gpu);
  printf("Vector Size: %d\n", N);
  printf("nThreads: %d\n", nThreads);
  printf("nBlocks: %d\n", nBlocks);
  printf("Total Time %4.6f ms\n", elapsedTime);
  printf("Bandwidth %4.3f GB/s\n",
         (N * sizeof(double)) / (1000000 * elapsedTime));

  cudaEventDestroy(start);
  cudaEventDestroy(stop);

  t1 = GetTime();
  test = Test(N, h_v, SUM, &SumSeq);
  t2 = GetTime();
  SeqTime = t2 - t1;
  printf("Speedup: x%2.3f \n", SeqTime / elapsedTime);

  if (test)
    printf("TEST PASS, Time seq: %f ms\n", SeqTime);
  else {
    printf("ERROR: %f(GPU) : %f(CPU) : %f(diff) : %f(error) \n", SumSeq, SUM,
           abs(SumSeq - SUM), abs(SumSeq - SUM) / SumSeq);
    printf("TEST FAIL\n");
  }
}
